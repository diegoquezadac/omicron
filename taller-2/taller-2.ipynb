{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5rfZ_hYaQ1Z"
   },
   "source": [
    "<center><img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"30%\" /></center>\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-398 Aprendizaje Automático </h1>\n",
    "\n",
    "<H3 align='center'> Tarea/Taller 2 </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KovasDMUMnW"
   },
   "source": [
    "# Temas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUgiKCl3UREw"
   },
   "source": [
    "* Regresión Lineal\n",
    "* Regularización\n",
    "* Selección de Características\n",
    "* Reducción de Dimensionalidad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hHigypDURhb"
   },
   "source": [
    "# Reglas & Formalidades\n",
    "\n",
    "* Pueden trabajar en equipos de 2 a 3 personas. \n",
    "* Los equipos deben ser inscritos antes del 10 de Noviembre.\n",
    "* Pueden reusar código visto en clases y/o recolectar código/ideas de otros sitios, mencionando al autor y entregando un link a la fuente. \n",
    "* Si resulta necesaria, la intervención de personas ajenas al grupo (e.g. experto) debe ser declarada y justificada.\n",
    "* Tener roles dentro del equipo está bien, pero al final del proceso, cada miembro debe entender y estar en condiciones de exponer todo el trabajo realizado. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQeZ0_M7Ucf-"
   },
   "source": [
    "# Entregables \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DOvGvOZUZYV"
   },
   "source": [
    "\n",
    "> * **Video:** Se debe preparar un video explicativo de **15 a 20 minutos** donde se describe la metodología utilizada, los resultados obtenidos y las conclusiones de la experiencia. \n",
    "\n",
    "> * **Código:** Se debe enviar un jupyter notebook con el código utilizado, de modo que sea posible **reproducir los resultados** presentados. Como alternativa, se puede entregar un link Github con el código fuente, incluyendo instrucciones precisas para ejecutar los experimentos. En cualquier caso (notebook o repo) el código debe estar ordenado y seccionado apropiadamente.\n",
    "\n",
    "> * **Conformidad Ética:** Se debe incluir una breve declaración ética en que se indique que el trabajo que se está enviando es un trabajo original, desarollado por los autores en conformidad con todas reglas antes mencionadas. Se debe también mencionar brevemente cuál fue la contribución de cada miembro del equipo. La declaración puede ser parte del notebook o estar en un archivo dentro del repo.\n",
    "\n",
    "> * **Defensa en vivo (video-conferencia):** El día de clases agendado para la discusión del taller, se seleccionarán aleatoriamente algunos equipos que presentarán oralmente su trabajo ante el curso. Los autores serán evaluados considerando la discusión y debate que generen entre sus pares. Los puntos obtenidos (positivos o negativos) se sumarán a la nota final de taller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNyRw2PdUgaA"
   },
   "source": [
    "# Fechas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lmzliloUhux"
   },
   "source": [
    "> * Defensas: 26 de Noviembre, horario de clases.\n",
    "> * Fecha de entrega de vídeo: 27 de Noviembre 23:59 Hrs. (1 días después de encuentro).\n",
    "> * Fecha de entrega de Jupyter (notebook): 26 de Noviembre 08:00 (se pueden hacer actualizaciones hasta el 27 de Noviembre 23:59 Hrs.). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnZJyoJGUmf9"
   },
   "source": [
    "# Instrucciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MF1q0Ei3Un6K"
   },
   "source": [
    "La tarea se divide en dos secciones:\n",
    "\n",
    "\n",
    "\n",
    "> **1. Pregunta de Investigación**. Para esta parte, los autores deben elegir una hipótesis de investigación y diseñar un procedimiento experimental que permita reunir evidencia en contra o a favor de la misma. Es legítimo tomar una posición *a-priori* en base a lo que han aprendido en el curso, pero es importante analizar críticamente los resultados sin descartar hipótesis alternativas. \n",
    "\n",
    "> La metodología debe incluir al menos 3 datasets, de los cuales al menos 2 deben ser reales. Es deseable también que incluyan experimentos controlados sobre dataset sintéticos o semi-sintéticos no triviales diseñados por ustedes. Por ejemplo, para demostrar que un método logra ignorar variables irrelevantes se podrían crear variables \"fake\" manualmente. Experimentos de este último tipo que se basen en un dataset real contarán como realizados sobre \"dataset reales\".\n",
    "\n",
    "> Si no es relevante para la pregunta de investigación y en honor al tiempo, no es necesario llevar a cabo un análisis exploratorio detallado sobre cada dataset utilizado.\n",
    "\n",
    ">  **2. Desafío Kaggle**. Para esta parte, los autores enfrentarán un desafío en la plataforma Kaggle y serán calificados en base a su posición en el tablero de resultados y el puntaje obtenido.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgDmUARIUw0w"
   },
   "source": [
    "# Parte 1. Pregunta de Investigación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTkbRyusPMok"
   },
   "source": [
    "> **1. Regresión Lineal:** Expandir un modelo de regresión lineal con características polinomiales reduce siempre el error de entrenamiento pero no necesariamente el error de predicción.\n",
    "\n",
    "> **2. Regresión Lineal:** Al entrenar un modelo de regresión lineal con pocos ejemplos y muchas características ($d > n$), la varianza de los estimadores de mínimos cuadrados aumenta y de consecuencia aumenta también la varianza de las predicciones que hace el modelo obtenido.\n",
    "\n",
    "> **3. Regresión Lineal:** En un problema de regresión en que la respuesta (y) tiene una gran cantidad de \\emph{outliers}, entrenar el modelo lineal con una función de costo (loss) más robusta que el error cuadrático (como la función de Huber) permite mejorar el error de predicción.\n",
    "\n",
    "> **4. Regresión Lineal:** Filtrar los atributos con muchos outliers antes de entrenar un modelo lineal de regresión permite mejorar el error de predicción.\n",
    "\n",
    "> **5. Regularización:** Sabemos que entrenar un modelo lineal agregando ruido blanco a los atributos es matemáticamente equivalente a usar el regularizador $L_2$. En la práctica sin embargo, el primer método resulta mucho más efectivo. \n",
    "\n",
    "> **6. Regularización:** Al entrenar un modelo lineal sobre texto, usando bigramas y trigramas, resulta fundamental equipar el modelo con un regularizador para prevenir overfitting.\n",
    "\n",
    "> **7. Regularización:** Usar el regularizador $L_2$ en un modelo lineal de clasificación (como el \\emph{regresor logístico}) resulta más efectivo y más eficiente que usar el regularizador $L_1$.\n",
    "\n",
    "> **8. Regularización:** Combinar el regularizador $L_2$ con el regularizador $L_1$ en un modelo lineal de regresión resulta más efectivo que cualquiera de las dos técnicas por separado.\n",
    "\n",
    "> **9. Regularización:** Al momento de elegir el valor del parámetro de regularización mediante validación cruzada, la regla denominada ``one-standard-error rule'' permite obtener modelos lineales más dispersos sin deteriorar significativamente el error de predicción.  \n",
    "\n",
    "> **10. Regularización:** Al computar el ``regularization path'' para el Lasso o Ridge Regression, la curva que corresponde a cada coeficiente es monótona y, en particular, no cambia de signo. Lo mismo ocurre con el error de validación cruzada correspondiente a los diferentes modelos obtenidos. \n",
    "\n",
    "> **11. Regularización:** En situaciones de fuerte co-linealidad entre atributos, el Lasso selecciona sólo una de las variables correlacionadas mientras que Ridge Regression mantiene todas las variables en el modelo reduciendo su peso de forma equitativa.\n",
    "\n",
    "> **12. Reducción de Dimensionalidad**: Combinando el modelo lineal de regresión con PCA perdemos en términos de interpretabilidad pero mejoramos significativamente el error de predicción. \n",
    "\n",
    "> **13. Reducción de Dimensionalidad**: Entrenar un clasificador sobre una representación de menor dimensionalidad obtenida vía LDA resulta más efectivo que hacerlo sobre una representación encontrada vía PCA, aún si a la última se le permite utilizar más dimensiones.\n",
    "\n",
    "> **14. Reducción de Dimensionalidad***: Un método lineal de reducción de dimensionalidad como PCA o LDA es mucho menos efectivo que un método no-lineales como TSNE para preservar la estructura de clases original. Esto resulta evidente al visualizar las proyecciones obtenidas y también al entrenar un clasificador sobre la nueva representación.\n",
    "\n",
    "> **15. Selección de Características:** Para el modelo de regresión lineal, un filtrado individual de atributos basado en el $F$-score permite encontrar las $K$ variables que minimizan el error de predicción.\n",
    "\n",
    "> **16. Selección de Características:** Seleccionar atributos para un problema de clasificación mediante el criterio denominado $\\chi^2$ resulta más efectivo que hacerlo usando RELIEF. \n",
    "\n",
    "> **17. Selección de Características:** Seleccionar atributos para un problema de clasificación mediante el método denominado MRMR (Minimum-Redundancy-Maximum-Relevance) resulta más efectivo que hacerlo filtrando vía Información Mutual individual. \n",
    "\n",
    "> Nota: Hay varias implementaciones públicas de MRMR compatibles con Sklearn. Ver por ejemplo emph{https://github.com/smazzanti/mrmr}\n",
    "\n",
    "> **18. Selección de Características:** Cuando se utiliza Forward Stepwise Selection junto a un modelo lineal de regresión, la estrategia de elegir la variable a incorporar re-entrenando el modelo y estimando su error de predicción, resulta notablemente más efectiva que hacer la elección usando un estimador de la correlación lineal de las variables con la respuesta (y).\n",
    "\n",
    "> **19. Selección de Características:** En problemas de clasificación con datos altamente dimensionales (al menos varias decenas de atributos), el método RELIEF es más efectivo y eficiente que el método denominado MRMR (Minimum-Redundancy-Maximum-Relevance)\n",
    "\n",
    "> Nota: Hay varias implementaciones públicas de MRMR compatibles con Sklearn. Ver por ejemplo emph{https://github.com/smazzanti/mrmr}\n",
    "\n",
    "\n",
    "> **20. Selección de Características:** Al entrenar un modelo lineal sobre texto, seleccionar atributos mediante el criterio denominado $\\chi^2$ resulta mucho más efectivo que hacerlo vía RELIEF.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mro0DvciPO8t"
   },
   "source": [
    "# Parte 2. Desafío Kaggle \n",
    "\n",
    "Una de las paradojas más bizarras e interesantes de esta pandemia ha sido la masificación del concepto nómade digital: una persona, generalmente joven, que se pasa la vida viajando y trabajando de manera remota, a veces desde lugares inusuales y paradisíacos. En palabras de la multitud de personas que durante el último año se han sumado a este estilo de vida, la razón es tan simple como envidiable: \"Si tenemos que trabajar en línea, ¿porqué no hacerlo desde un país exótico?\".  \n",
    "\n",
    "Para esta parte del taller, tendrán que constuir un modelo capaz de predecir el precio de un paquete de viajes orientado a nómades digitales a partir de una serie de detalles como el destino o itinerario, los hoteles y comidas incluídas, los vuelos involucrados, etc. \n",
    "\n",
    "Para acceder a loss detalles del desafío ingrese al siguiente link: https://www.kaggle.com/c/nomades-digitales/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVzSTfeQkrBl"
   },
   "source": [
    "## 1. Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fundamentals\n",
    "import numpy as np\n",
    "import spacy\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Descripción Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recopilación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train and test sets\n",
    "train = pd.read_csv(\"./datasets/travel_packages_train.csv\")\n",
    "test = pd.read_csv(\"./datasets/travel_packages_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix columns names\n",
    "train = train.rename(columns = {\"Unnamed: 0\": \"Index\"})\n",
    "train.columns = train.columns.str.lower().str.replace(' ','_')\n",
    "test = test.rename(columns = {\"Unnamed: 0\": \"Index\"})\n",
    "test.columns = test.columns.str.lower().str.replace(' ','_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploración de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15750, 15)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'package_name', 'package_type', 'destination', 'itinerary',\n",
      "       'places_covered', 'travel_date', 'hotel_details', 'start_city',\n",
      "       'airline', 'flight_stops', 'meals', 'sightseeing_places_covered',\n",
      "       'cancellation_rules', 'ppprice'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Column names\n",
    "print(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index                         15750\n",
      "package_name                   2171\n",
      "package_type                      5\n",
      "destination                     563\n",
      "itinerary                       964\n",
      "places_covered                  563\n",
      "travel_date                     481\n",
      "hotel_details                  5158\n",
      "start_city                        2\n",
      "airline                         272\n",
      "flight_stops                      3\n",
      "meals                             4\n",
      "sightseeing_places_covered     1650\n",
      "cancellation_rules                8\n",
      "ppprice                         701\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Number of uniques values per column\n",
    "print(train.nunique(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>package_name</th>\n",
       "      <th>package_type</th>\n",
       "      <th>destination</th>\n",
       "      <th>itinerary</th>\n",
       "      <th>places_covered</th>\n",
       "      <th>travel_date</th>\n",
       "      <th>hotel_details</th>\n",
       "      <th>start_city</th>\n",
       "      <th>airline</th>\n",
       "      <th>flight_stops</th>\n",
       "      <th>meals</th>\n",
       "      <th>sightseeing_places_covered</th>\n",
       "      <th>cancellation_rules</th>\n",
       "      <th>ppprice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7919</td>\n",
       "      <td>North East - Gangtok and Lachung</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>Gangtok|Lachung|Gangtok</td>\n",
       "      <td>1N Gangtok . 2N Lachung . 1N Gangtok</td>\n",
       "      <td>Gangtok|Lachung|Gangtok</td>\n",
       "      <td>10-01-2022</td>\n",
       "      <td>Summit Denzong Hotel &amp; Spa:4.5|Summit Alpine R...</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>Spicejet|Spicejet</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Singhik View Point | Chungthang | Yumthang Va...</td>\n",
       "      <td>Cancellation any time after making the 1st pay...</td>\n",
       "      <td>284.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>982</td>\n",
       "      <td>Spiritual Haridwar from Delhi</td>\n",
       "      <td>Budget</td>\n",
       "      <td>Haridwar</td>\n",
       "      <td>2N Haridwar</td>\n",
       "      <td>Haridwar</td>\n",
       "      <td>21-11-2021</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Har Ki Pauri</td>\n",
       "      <td>Cancellation any time after making the 1st pay...</td>\n",
       "      <td>129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3211</td>\n",
       "      <td>Rajasthan Adventure Special with Guided Leopar...</td>\n",
       "      <td>Premium</td>\n",
       "      <td>Jodhpur|Jaisalmer</td>\n",
       "      <td>2N Jodhpur . 2N Jaisalmer</td>\n",
       "      <td>Jodhpur|Jaisalmer</td>\n",
       "      <td>06-08-2021</td>\n",
       "      <td>Indana Palace Jodhpur:4.3|Jaisalmer Marriott R...</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>IndiGo</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Hanwant Mahal | Candlelight Dinner in Jodhpur...</td>\n",
       "      <td>Cancellation any time after making the 1st pay...</td>\n",
       "      <td>311.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3509</td>\n",
       "      <td>Bhutan 5N - Thimphu(2) Paro(3)</td>\n",
       "      <td>Premium</td>\n",
       "      <td>Thimpu|Paro</td>\n",
       "      <td>2N Thimpu . 3N Paro</td>\n",
       "      <td>Thimpu|Paro</td>\n",
       "      <td>27-02-2021</td>\n",
       "      <td>Terma Linca Resort and Spa:4.5|Zhiwa Ling Hote...</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Simtokha Dzong | Memorial Chorten | Buddha Do...</td>\n",
       "      <td>Cancellation any time after making the 1st pay...</td>\n",
       "      <td>517.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4820</td>\n",
       "      <td>Splendid North East - Pelling Special (Value a...</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>Gangtok|Pelling|Darjeeling</td>\n",
       "      <td>2N Gangtok . 1N Pelling . 2N Darjeeling</td>\n",
       "      <td>Gangtok|Pelling|Darjeeling</td>\n",
       "      <td>16-06-2021</td>\n",
       "      <td>Lemon Tree Hotel  Gangtok:4.3|The Elgin Mount ...</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>Go Air</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Changu Lake - Excursion | Baba Mandir | Tashi...</td>\n",
       "      <td>Cancellation any time after making the 1st pay...</td>\n",
       "      <td>336.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                       package_name package_type  \\\n",
       "0   7919                   North East - Gangtok and Lachung       Deluxe   \n",
       "1    982                      Spiritual Haridwar from Delhi       Budget   \n",
       "2   3211  Rajasthan Adventure Special with Guided Leopar...      Premium   \n",
       "3   3509                     Bhutan 5N - Thimphu(2) Paro(3)      Premium   \n",
       "4   4820  Splendid North East - Pelling Special (Value a...       Deluxe   \n",
       "\n",
       "                  destination                                itinerary  \\\n",
       "0     Gangtok|Lachung|Gangtok     1N Gangtok . 2N Lachung . 1N Gangtok   \n",
       "1                    Haridwar                              2N Haridwar   \n",
       "2           Jodhpur|Jaisalmer                2N Jodhpur . 2N Jaisalmer   \n",
       "3                 Thimpu|Paro                      2N Thimpu . 3N Paro   \n",
       "4  Gangtok|Pelling|Darjeeling  2N Gangtok . 1N Pelling . 2N Darjeeling   \n",
       "\n",
       "               places_covered travel_date  \\\n",
       "0     Gangtok|Lachung|Gangtok  10-01-2022   \n",
       "1                    Haridwar  21-11-2021   \n",
       "2           Jodhpur|Jaisalmer  06-08-2021   \n",
       "3                 Thimpu|Paro  27-02-2021   \n",
       "4  Gangtok|Pelling|Darjeeling  16-06-2021   \n",
       "\n",
       "                                       hotel_details start_city  \\\n",
       "0  Summit Denzong Hotel & Spa:4.5|Summit Alpine R...  New Delhi   \n",
       "1                                      Not Available     Mumbai   \n",
       "2  Indana Palace Jodhpur:4.3|Jaisalmer Marriott R...  New Delhi   \n",
       "3  Terma Linca Resort and Spa:4.5|Zhiwa Ling Hote...  New Delhi   \n",
       "4  Lemon Tree Hotel  Gangtok:4.3|The Elgin Mount ...  New Delhi   \n",
       "\n",
       "             airline  flight_stops  meals  \\\n",
       "0  Spicejet|Spicejet             1      3   \n",
       "1      Not Available             2      2   \n",
       "2             IndiGo             0      5   \n",
       "3      Not Available             0      5   \n",
       "4             Go Air             1      3   \n",
       "\n",
       "                          sightseeing_places_covered  \\\n",
       "0   Singhik View Point | Chungthang | Yumthang Va...   \n",
       "1                                      Har Ki Pauri    \n",
       "2   Hanwant Mahal | Candlelight Dinner in Jodhpur...   \n",
       "3   Simtokha Dzong | Memorial Chorten | Buddha Do...   \n",
       "4   Changu Lake - Excursion | Baba Mandir | Tashi...   \n",
       "\n",
       "                                  cancellation_rules  ppprice  \n",
       "0  Cancellation any time after making the 1st pay...    284.0  \n",
       "1  Cancellation any time after making the 1st pay...    129.0  \n",
       "2  Cancellation any time after making the 1st pay...    311.0  \n",
       "3  Cancellation any time after making the 1st pay...    517.0  \n",
       "4  Cancellation any time after making the 1st pay...    336.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Data integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file with distances\n",
    "with open('distances.json', 'r') as fp:\n",
    "    distancias = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3220.07333299853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-a41427ecd320>:5: RuntimeWarning: Mean of empty slice\n",
      "  if(np.isnan(np.nanmean(np.array(list(destinos.values()), dtype=np.float64)))):\n"
     ]
    }
   ],
   "source": [
    "# Get average distance between two places\n",
    "promedio = 0\n",
    "for origen, destinos in distancias.items():\n",
    "    \n",
    "    if(np.isnan(np.nanmean(np.array(list(destinos.values()), dtype=np.float64)))):\n",
    "        continue\n",
    "    else:\n",
    "        promedio = (promedio + np.nanmean(np.array(list(destinos.values()), dtype=np.float64)))/2\n",
    "\n",
    "for origen, destinos in distancias.items():\n",
    "    # Distancia promedio del origen a todos los destinos\n",
    "    for destino, distancia in destinos.items():\n",
    "        if(distancia == None or np.isnan(distancia)):\n",
    "            distancias[origen][destino] = promedio\n",
    "print(promedio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform travel_date format to datetime\n",
    "train['date'] = pd.to_datetime(train['travel_date'])\n",
    "test['date'] = pd.to_datetime(test['travel_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform datetime to ordinal representation\n",
    "train['date'] = train['date'].map(dt.datetime.toordinal)\n",
    "test['date'] = test['date'].map(dt.datetime.toordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of places included in package\n",
    "train[\"number_places\"] = train.apply(lambda row: len(row.places_covered.split(\"|\")),axis=1)\n",
    "test[\"number_places\"] = test.apply(lambda row: len(row.places_covered.split(\"|\")),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of sightseeings (visits to tourist places)\n",
    "train[\"number_sightseeing\"] = train.apply(lambda row: len(row.sightseeing_places_covered.split(\"|\")),axis=1)\n",
    "test[\"number_sightseeing\"] = test.apply(lambda row: len(row.sightseeing_places_covered.split(\"|\")),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train[\"number_destination\"] = train.apply(lambda row: len(row.destination.split(\"|\")),axis=1)\n",
    "#test[\"number_destination\"] = test.apply(lambda row: len(row.destination.split(\"|\")),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode package type as numbers\n",
    "package_codes = {'Budget': 1, 'Standard': 4, 'Premium': 9, 'Deluxe': 16,'Luxury':25}\n",
    "train[\"package_code\"] = train.apply(lambda row: package_codes[row.package_type],axis=1)\n",
    "test[\"package_code\"] = test.apply(lambda row: package_codes[row.package_type],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get meal quality score based on the package\n",
    "train[\"meals_score\"] = train[\"meals\"] * train[\"package_code\"]\n",
    "test[\"meals_score\"] = test[\"meals\"] * train[\"package_code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of nights\n",
    "train[\"nights\"] = train.apply(lambda row: sum(map(int,re.findall(\"(\\d+)N\",row.itinerary))),axis=1)\n",
    "test[\"nights\"] = test.apply(lambda row: sum(map(int,re.findall(\"(\\d+)N\",row.itinerary))),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of hotels with spa\n",
    "train[\"spa\"] = train.apply(lambda row: int(\"spa\" in row.hotel_details.lower()),axis=1)\n",
    "test[\"spa\"] = test.apply(lambda row: int(\"spa\" in row.hotel_details.lower()),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_spa(itinerary, hotel_details, nights):\n",
    "    try:\n",
    "        return sum(np.array(list(map(int,re.findall(\"(\\d+)N\",itinerary)))) * np.array((list((\"spa\" in s) for s in hotel_details.lower().split(\"|\")))))\n",
    "    except:\n",
    "        return nights/2\n",
    "def weighted_mean_rate(itinerary, hotel_details):\n",
    "    itinerario = np.array(list(map(int,re.findall(\"(\\d+)N\",itinerary))))\n",
    "    rates = np.array(list(map(float,re.findall(\"(\\d\\.\\d)\",hotel_details))))\n",
    "    try:\n",
    "        return sum(itinerario * rates)/sum(itinerario)\n",
    "    except:\n",
    "        return np.mean(rates) if len(rates)>0 else 0 # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of nights with spa (estimated sometimes)\n",
    "train[\"nights_with_spa\"] = train.apply(lambda row: count_spa(row.itinerary, row.hotel_details, row.nights),axis=1)\n",
    "test[\"nights_with_spa\"] = test.apply(lambda row: count_spa(row.itinerary, row.hotel_details, row.nights),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hotel quality mean weighted by number of nights in hotels \n",
    "train[\"hotels_score\"] = train.apply(lambda row: weighted_mean_rate(row.itinerary, row.hotel_details),axis=1)\n",
    "test[\"hotels_score\"] = test.apply(lambda row: weighted_mean_rate(row.itinerary, row.hotel_details),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode start city\n",
    "train = train.join(pd.get_dummies(train['start_city']))\n",
    "test = test.join(pd.get_dummies(test['start_city']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode package of cancellation rules\n",
    "cancellation_rules_code = dict(zip(train[\"cancellation_rules\"].unique(), ['a','b','c','d','e','f','g','h']))\n",
    "\n",
    "train[\"cancellation_rule_package\"] = train.apply(lambda row: cancellation_rules_code.get(row.cancellation_rules,\"i\"),axis=1)\n",
    "test[\"cancellation_rule_package\"] = test.apply(lambda row: cancellation_rules_code.get(row.cancellation_rules,\"i\"),axis=1)\n",
    "\n",
    "train = train.join(pd.get_dummies(train['cancellation_rule_package']))\n",
    "train[\"i\"] = 0\n",
    "\n",
    "test = test.join(pd.get_dummies(test['cancellation_rule_package']))\n",
    "test[\"e\"] = 0\n",
    "test[\"g\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_distance(destinations, distances):\n",
    "    distance = 0\n",
    "    for i in range(len(destinations)-1):\n",
    "        current_distance = distances[destinations[i]][destinations[i+1]] if ((destinations[i] in distances and destinations[i+1] in distances[destinations[i]])) else distances[destinations[i+1]][destinations[i]]\n",
    "        distance = distance + current_distance\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total distance\n",
    "train[\"total_distance\"] = train.apply(lambda row: total_distance(np.append(np.append(row.start_city,np.array(row.destination.split(\"|\"))),row.start_city),distancias),axis=1)\n",
    "test[\"total_distance\"] = test.apply(lambda row: total_distance(np.append(np.append(row.start_city,np.array(row.destination.split(\"|\"))),row.start_city),distancias),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the date due to its cyclical nature\n",
    "train[\"sin_date\"] = train.apply(lambda row: np.sin(2*np.pi*(int(row.travel_date.split(\"-\")[0])+(int(row.travel_date.split(\"-\")[1])*30))/365),axis=1)\n",
    "train[\"cos_date\"] = train.apply(lambda row: np.cos(2*np.pi*(int(row.travel_date.split(\"-\")[0])+(int(row.travel_date.split(\"-\")[1])*30))/365),axis=1)\n",
    "\n",
    "test[\"sin_date\"] = test.apply(lambda row: np.sin(2*np.pi*(int(row.travel_date.split(\"-\")[0])+(int(row.travel_date.split(\"-\")[1])*30))/365),axis=1)\n",
    "test[\"cos_date\"] = test.apply(lambda row: np.cos(2*np.pi*(int(row.travel_date.split(\"-\")[0])+(int(row.travel_date.split(\"-\")[1])*30))/365),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Selección de modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['package_code', \n",
    "                      'meals_score','spa','nights','nights_with_spa','hotels_score', \n",
    "                      'a','b','c','d','e','f','g','h','i','Mumbai','New Delhi',\n",
    "                      'meals','flight_stops','total_distance','sin_date','cos_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train[numerical_features], train['ppprice'],test_size=0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "columns_to_normalize = ['package_code','meals_score','nights', 'nights_with_spa', 'hotels_score','meals','flight_stops','total_distance']\n",
    "ct = ColumnTransformer([\n",
    "        ('standardscal', StandardScaler(), columns_to_normalize)\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "X_train = ct.fit_transform(X_train)\n",
    "X_val = ct.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical_features = ['package_code', \n",
    "#                      'spa','nights','hotels_score', \n",
    "#                      'a','b','c','d','e','f','h','New Delhi',\n",
    "#                      'meals','total_distance','sin_date','cos_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "#columns_to_normalize = ['package_code','nights', 'hotels_score','meals','total_distance']\n",
    "#ct = ColumnTransformer([\n",
    "#        ('standardscal', StandardScaler(), columns_to_normalize)\n",
    "#    ], remainder='passthrough')\n",
    "#\n",
    "#X_train = ct.fit_transform(X_train)\n",
    "#X_val = ct.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18248479647718374"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr = lr.fit(X_train,y_train)\n",
    "y_pred = lr.predict(X_val)\n",
    "mean_absolute_percentage_error(y_val,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18247571550311628"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "lasso = linear_model.Lasso(alpha=0.001)\n",
    "lasso = lasso.fit(X_train,y_train)\n",
    "y_pred = lasso.predict(X_val)\n",
    "mean_absolute_percentage_error(y_val,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18246762419908813"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_ridge = Ridge()\n",
    "lr_ridge = lr_ridge.fit(X_train, y_train)\n",
    "y_pred = lr_ridge.predict(X_val)\n",
    "mean_absolute_percentage_error(y_val,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test[numerical_features]\n",
    "X_test = ct.fit_transform(X_test)\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['ppprice'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[['index','ppprice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.rename(columns={'index':'Index','ppprice':'PPPrice'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>PPPrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7798</td>\n",
       "      <td>374.823458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1788</td>\n",
       "      <td>226.565141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2084</td>\n",
       "      <td>306.050010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18361</td>\n",
       "      <td>262.636902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13939</td>\n",
       "      <td>372.551658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5245</th>\n",
       "      <td>19911</td>\n",
       "      <td>279.142507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5246</th>\n",
       "      <td>13668</td>\n",
       "      <td>366.843208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5247</th>\n",
       "      <td>12168</td>\n",
       "      <td>267.804305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5248</th>\n",
       "      <td>20634</td>\n",
       "      <td>223.612331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5249</th>\n",
       "      <td>2575</td>\n",
       "      <td>440.403714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5250 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Index     PPPrice\n",
       "0      7798  374.823458\n",
       "1      1788  226.565141\n",
       "2      2084  306.050010\n",
       "3     18361  262.636902\n",
       "4     13939  372.551658\n",
       "...     ...         ...\n",
       "5245  19911  279.142507\n",
       "5246  13668  366.843208\n",
       "5247  12168  267.804305\n",
       "5248  20634  223.612331\n",
       "5249   2575  440.403714\n",
       "\n",
       "[5250 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"./submissions/submission-2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conformidad ética"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Contribución por integrante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Codificar destinations.\n",
    "2. Verificar normalidad de los atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[INF398]Taller2_II_2021.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
